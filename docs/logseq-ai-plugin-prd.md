# Logseq AI 插件 PRD

## 1. 背景

- Logseq 用户希望在笔记环境中快速调用本地或远程大模型完成写作、总结、任务拆解等操作。
- 现有 `ollama-logseq` 插件提供了基础命令面板、右键菜单和 block 属性等能力，但在可配置性、错误处理和体验细节上仍有改进空间。
- 新插件目标是在保持轻量安装的前提下，提供更稳健、可扩展、可定制的 AI 交互体验。

## 2. 目标

1. 支持自由提问、上下文问答（页面/块）、摘要、任务拆解、闪卡生成等常用操作，且易于扩展自定义命令。
2. 提供一致的命令入口（命令面板/Slash Command/右键菜单/工具栏按钮），并能根据主题自动适配样式。
3. 允许通过设置面板与 block 属性灵活配置模型、提示词、上下文范围等参数。
4. 优化 UX：提供流式反馈/进度提示、错误兜底及无选中块时的 fallback 行为。
5. 具备良好的可维护性：统一的 LLM 请求封装、可测试的纯逻辑函数、详尽文档与演示。

## 3. 用户画像 & 场景

- **研究型用户**：在项目笔记内快速总结页面、提炼关键点。
- **学习者**：将课程内容转换为闪卡，或对块内容提出问题、扩展思路。
- **知识工作者**：拆解 TODO 任务、编写会议纪要、翻译/润色笔记。
- **高级用户**：希望定制模型/提示词/上下文命令，甚至接入私有模型服务。

场景示例：

1. 按下快捷键 → 打开命令面板 → 选择“Ask with page context” → 输入问题 → 在当前块下生成答案，并保留引用上下文。
2. 选中一个 TODO → 右键选择“Divide into subtasks” → 插件按照缩进生成结构化子任务。
3. 在配置页 `ai-command-config` 中新增属性 `ai-context-menu-title`、`ai-prompt-prefix` 的块 → 自动生成一个新的上下文菜单命令。
4. 在块尾添加 `ai-generate-model:: qwen2` → 该块及子块的操作自动切换到指定模型。

## 4. 功能范围

### 4.1 基础能力

- 安装后自动注册：
  - 工具栏按钮、快捷键、Slash Command `/ai`.
  - React 主 UI + Command Palette（支持输入过滤、热键 Esc/Enter 等）。
- 设置项：host、默认模型、API 路径、快捷键、自定义 prompt 列表、默认上下文长度限制、是否使用 streaming 等。
- Block 属性解析：识别 `ai-generate-*`（支持 `model`, `temperature`, `top_p`, `use_context` 等），自动继承父块。
- 配置页 `ai-command-config`：通过 block 属性新增上下文菜单命令，可指定 prompt 前缀/后缀、模型或其他参数。

### 4.2 预置命令

1. **Ask AI**：无上下文自由提问。
2. **Ask with page/block context**：自动拼接当前页面或块树内容，支持截断或智能摘要后再发送。
3. **Summarize page/block**：在目标位置插入占位文本，完成后回填摘要；可选 streaming 更新。
4. **Flash card**：生成问答两块，自动加 `#card` 标签。
5. **Divide task**：将 TODO 拆分为嵌套子任务，保留缩进结构。
6. **Custom prompt**：从设置面板或配置页读取自定义 Prompt，出现在命令面板和上下文菜单中。

### 4.3 UX 与可靠性

- 所有耗时操作都插入 placeholder（含状态或 spinner），并在失败时显示错误信息或提示重试。
- 命令面板与 Prompt 输入框区分问答类 vs 立即执行类命令；防止大小写/选项名不一致带来的触发失败。
- 统一 LLM 请求层：支持超时、重试、取消、streaming，可过滤 `<think>` 等额外内容。
- 当用户未选中块时，可选在页面末尾或收集箱页面创建新块作为输出位置。

## 5. 非目标

- 不实现完整的聊天会话面板（聚焦单次命令）。
- 不直接集成第三方云端模型（但保留接口以便用户自行配置）。
- 不负责 Logseq 版本兼容以外的笔记同步/管理功能。

## 6. 技术要点

- 使用 Vite + React + TypeScript，复用 Logseq 插件模板。
- 抽象 `LLMClient`（支持 Ollama、本地 HTTP 模型或可配置 endpoint），集中处理 headers/参数/stream decoding。
- `getTreeContent` 改进：保留层级缩进、过滤属性行、支持最大 token 限制（可依赖 heuristics/估算）。
- Streaming：若启用，对应 block 使用 `updateBlock` 逐步写入；需要处理用户取消/窗口关闭事件。
- 错误处理：`try/catch` + `logseq.App.showMsg`，并记录到 console，必要时建议用户检查设置。

## 7. 时间线 & 里程碑（示例）

1. **Week 1**：定义数据结构、完成设置面板、命令注册骨架。
2. **Week 2**：实现 LLM 客户端、Ask/Summarize/Flash card 功能，支持 block 属性与配置页。
3. **Week 3**：完善命令面板 UX、上下文管理（截断/格式化），新增拆任务与自定义命令。
4. **Week 4**：增强 streaming、错误处理，补充 README/GIF/PRD 校对，准备首次发布。

## 8. 成功指标

- 安装后 5 分钟内新用户即可完成首次 AI 命令（通过文档与 UI 流程验证）。
- 常见操作（Ask/Summarize/Divide）成功率 ≥95%，失败时有明确提示。
- 自定义命令配置/Block 属性使用频率 ≥30% 的活跃用户至少用到一次（反映灵活性）。
- GitHub issues 中与“命令找不到/无反馈”相关的问题低于 5%。

## 9. 未来拓展

- 支持多模型切换 UI、模型权重管理或自动下载。
- 增加模板变量 / 多块合成 prompt 的高级功能。
- 提供会话上下文、引用插入、批量操作等高级模式。
